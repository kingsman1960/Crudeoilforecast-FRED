{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Acquisition\n",
    "  ## Overview\n",
    "    \n",
    "    \"All data is at the daily level, represented as a volume weighted average.  Data is acquired all the way back to 2011, the longest period to obtain a reasonably complete dataset.  \\n\",\n",
    "  \\\n",
    "    \"The sections below will constitute a data dictionary for the columns utilized in this inquiry.\\n\",\n",
    "   \n",
    " ## US Equity Indices,\n",
    "  \n",
    "    Given the importance of equity markets to the health of the overall economy, as well as the media's obsession with their movements, daily time-series of the following were included:\n",
    "    \n",
    "   - SP500: [SPX S&P 500 Index](https://us.spindices.com/indices/equity/sp-500) of large-cap US equities\\n\",\n",
    "   - NASDAQCOM: [Nasdaq Composite Index](http://money.cnn.com/data/markets/nasdaq/) of large-cap US equities\\n\",\n",
    "    DJIA: [Dow Jones Industrial Average](https://quotes.wsj.com/index/DJIA) of US equities\\n\",\n",
    "   - RU2000PR: [Russell 2000 Price Index](https://fred.stlouisfed.org/series/RU2000PR) of US equities\\n\",\n",
    "\n",
    " The `pandas_datareader.data` and `quandl` APIs were used to acquire this information.\n",
    "\n",
    " ## Traditional Currencies\\n\n",
    "\n",
    "The [St. Louis Federal Reserve's FRED API](https://fred.stlouisfed.org/) was accessed using the [`pandas_datareader.data`](https://pandas-datareader.readthedocs.io/en/latest/) API to gather currency exchange rates of the US Dollar against the Japanese Yen, the Euro, the Chinese Yuan, the Mexican Peso, and the Australian Dollar\n",
    "    \n",
    " \"- DEXCHUS: [Chinese Yuan to USD](https://fred.stlouisfed.org/series/DEXCHUS)\\n\",\n",
    " \"- DEXJPUS: [Japanese Yen to USD](https://fred.stlouisfed.org/series/DEXJPUS)\\n\",\n",
    " - DEXUSEU: [USD to European Union's Euro](https://fred.stlouisfed.org/series/DEXUSEU)\\n\",\n",
    " \"- DEXMXUS: [Mexican New Pesos to USD](https://fred.stlouisfed.org/series/DEXMXUS)\\n\",\n",
    " \"- DEXUSAL: [USD to Australian Dollar](https://fred.stlouisfed.org/series/DEXUSAL)\\n\",\n",
    "    \n",
    " ## Debt Market Indicators\n",
    "    \n",
    "A ladder of bond market indicators are represented in the data in LIBOR rates at various maturities.  Specifically, LIBOR is included at overnight, 1-month, 3-month and 12-month maturities.  To (very crudely) represent the consumer and the corporate markets we also included indices representing high yield returns and prime corporate debt returns.\n",
    "  \n",
    " - USDONTD156N: [Overnight London Interbank Offered Rate (LIBOR](https://fred.stlouisfed.org/series/USDONTD156N) based on USD\\n\",\n",
    " - USD1MTD156N: [One Month London Interbank Offered Rate (LIBOR](https://fred.stlouisfed.org/series/USD1MTD156N) based on USD\\n\",\n",
    "- USD3MTD156N: [Three Month London Interbank Offered Rate (LIBOR](https://fred.stlouisfed.org/series/USD3MTD156N) based on USD\\n\",\n",
    "- USD12MD156N: [Twelve Month London Interbank Offered Rate (LIBOR](https://fred.stlouisfed.org/series/USD12MD156N) based on USD\\n\",\n",
    "- BAMLHYH0A0HYM2TRIV: [ICE BofAML US High Yield Total Return Index Value](https://fred.stlouisfed.org/series/BAMLHYH0A0HYM2TRIV)\\n\",\n",
    "- BAMLCC0A1AAATRIV: [ICE BofAML US Corp AAA Total Return Index Value](https://fred.stlouisfed.org/series/BAMLCC0A1AAATRIV)\n",
    "    These series were also acquired from the St. Louis Fed's FRED API.\n",
    "  \n",
    " ## Commodity Prices\\n\",\n",
    "  \n",
    " We chose to include series that represent the oil market and the gold market, two assets that are not strongly tied to the others mentioned.\n",
    "    \n",
    "- GOLDAMGBD228NLBM: [Gold Fixing Price 10:30 AM (London Time) in London Bullion Market, based on  USD](https://fred.stlouisfed.org/series/GOLDAMGBD228NLBM)\n",
    "  - DCOILWTICO: [West Texas Intermediate (WTI) - Cushing Oklahoma (https://fred.stlouisfed.org/series/DCOILWTICO)\n",
    "\n",
    "  \"These series were also acquired from the St. Louis Fed's FRED API.\n",
    "    \n",
    "  ## Energy-Related Series\n",
    "    \"To ensure we are getting signal from the energy sector data on natural gas and energy sector volatility is gathered.  This data is alos acquired form teh St. Louis Fed's FRED API.\n",
    "    \"\\n\",\n",
    "    \"- MHHNGSP: [Henry Hub Natural Gas Spot Price](https://fred.stlouisfed.org/series/MHHNGSP)\\n\",\n",
    "    \"- VXXLECLS: [CBOE Energy Sector ETF Volatility Index](https://fred.stlouisfed.org/series/VXXLECLS)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Call FRED API \\n\",\n",
    "    \"\\n\",\n",
    "    \"Below a simple function `get_fred_data` is defined to call the Saint Louis Fed's FRED API via pandas_datareader.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import quandl\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 999)\n",
    "pd.set_option('max_rows', 99999)\n",
    "\n",
    "from fredapi import Fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling FRED API for Series:  SP500\n",
      "Calling FRED API for Series:  NASDAQCOM\n",
      "Calling FRED API for Series:  DJIA\n",
      "Calling FRED API for Series:  BOGMBASEW\n",
      "Calling FRED API for Series:  DEXJPUS\n",
      "Calling FRED API for Series:  DEXUSEU\n",
      "Calling FRED API for Series:  DEXCHUS\n",
      "Calling FRED API for Series:  DEXUSAL\n",
      "Calling FRED API for Series:  VIXCLS\n",
      "Calling FRED API for Series:  USDONTD156N\n",
      "Calling FRED API for Series:  USD1MTD156N\n",
      "Calling FRED API for Series:  USD3MTD156N\n",
      "Calling FRED API for Series:  USD12MD156N\n",
      "Calling FRED API for Series:  BAMLHYH0A0HYM2TRIV\n",
      "Calling FRED API for Series:  BAMLCC0A1AAATRIV\n",
      "Calling FRED API for Series:  GOLDAMGBD228NLBM\n",
      "Calling FRED API for Series:  DCOILWTICO\n",
      "Calling FRED API for Series:  MHHNGSP\n",
      "Calling FRED API for Series:  VXXLECLS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQCOM</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>BOGMBASEW</th>\n",
       "      <th>DEXJPUS</th>\n",
       "      <th>DEXUSEU</th>\n",
       "      <th>DEXCHUS</th>\n",
       "      <th>DEXUSAL</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>USDONTD156N</th>\n",
       "      <th>USD1MTD156N</th>\n",
       "      <th>USD3MTD156N</th>\n",
       "      <th>USD12MD156N</th>\n",
       "      <th>BAMLHYH0A0HYM2TRIV</th>\n",
       "      <th>BAMLCC0A1AAATRIV</th>\n",
       "      <th>GOLDAMGBD228NLBM</th>\n",
       "      <th>DCOILWTICO</th>\n",
       "      <th>MHHNGSP</th>\n",
       "      <th>VXXLECLS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>2058.20</td>\n",
       "      <td>4726.81</td>\n",
       "      <td>17832.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.20</td>\n",
       "      <td>1.2015</td>\n",
       "      <td>6.2046</td>\n",
       "      <td>0.8118</td>\n",
       "      <td>17.79</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.16750</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>1048.41</td>\n",
       "      <td>573.03</td>\n",
       "      <td>1184.25</td>\n",
       "      <td>52.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>2020.58</td>\n",
       "      <td>4652.57</td>\n",
       "      <td>17501.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.64</td>\n",
       "      <td>1.1918</td>\n",
       "      <td>6.2201</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>19.92</td>\n",
       "      <td>0.1164</td>\n",
       "      <td>0.16800</td>\n",
       "      <td>0.2536</td>\n",
       "      <td>0.6338</td>\n",
       "      <td>1045.61</td>\n",
       "      <td>576.54</td>\n",
       "      <td>1192.00</td>\n",
       "      <td>50.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>2002.61</td>\n",
       "      <td>4592.74</td>\n",
       "      <td>17371.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.26</td>\n",
       "      <td>1.1936</td>\n",
       "      <td>6.2125</td>\n",
       "      <td>0.8118</td>\n",
       "      <td>21.12</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.16775</td>\n",
       "      <td>0.2511</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>580.14</td>\n",
       "      <td>1211.00</td>\n",
       "      <td>47.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>2025.90</td>\n",
       "      <td>4650.47</td>\n",
       "      <td>17584.52</td>\n",
       "      <td>3929023.0</td>\n",
       "      <td>119.52</td>\n",
       "      <td>1.1820</td>\n",
       "      <td>6.2127</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>19.31</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.16650</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.6273</td>\n",
       "      <td>1045.07</td>\n",
       "      <td>580.31</td>\n",
       "      <td>1213.75</td>\n",
       "      <td>48.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SP500  NASDAQCOM      DJIA  BOGMBASEW  DEXJPUS  DEXUSEU  \\\n",
       "DATE                                                                    \n",
       "2015-01-01      NaN        NaN       NaN        NaN      NaN      NaN   \n",
       "2015-01-02  2058.20    4726.81  17832.99        NaN   120.20   1.2015   \n",
       "2015-01-05  2020.58    4652.57  17501.65        NaN   119.64   1.1918   \n",
       "2015-01-06  2002.61    4592.74  17371.64        NaN   118.26   1.1936   \n",
       "2015-01-07  2025.90    4650.47  17584.52  3929023.0   119.52   1.1820   \n",
       "\n",
       "            DEXCHUS  DEXUSAL  VIXCLS  USDONTD156N  USD1MTD156N  USD3MTD156N  \\\n",
       "DATE                                                                          \n",
       "2015-01-01      NaN      NaN     NaN          NaN          NaN          NaN   \n",
       "2015-01-02   6.2046   0.8118   17.79       0.1126      0.16750       0.2556   \n",
       "2015-01-05   6.2201   0.8095   19.92       0.1164      0.16800       0.2536   \n",
       "2015-01-06   6.2125   0.8118   21.12       0.1187      0.16775       0.2511   \n",
       "2015-01-07   6.2127   0.8049   19.31       0.1189      0.16650       0.2521   \n",
       "\n",
       "            USD12MD156N  BAMLHYH0A0HYM2TRIV  BAMLCC0A1AAATRIV  \\\n",
       "DATE                                                            \n",
       "2015-01-01          NaN                 NaN               NaN   \n",
       "2015-01-02       0.6328             1048.41            573.03   \n",
       "2015-01-05       0.6338             1045.61            576.54   \n",
       "2015-01-06       0.6283             1042.00            580.14   \n",
       "2015-01-07       0.6273             1045.07            580.31   \n",
       "\n",
       "            GOLDAMGBD228NLBM  DCOILWTICO  MHHNGSP  VXXLECLS  \n",
       "DATE                                                         \n",
       "2015-01-01               NaN         NaN     2.99       NaN  \n",
       "2015-01-02           1184.25       52.72      NaN     29.81  \n",
       "2015-01-05           1192.00       50.05      NaN     32.37  \n",
       "2015-01-06           1211.00       47.98      NaN     33.31  \n",
       "2015-01-07           1213.75       48.69      NaN     32.05  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "series_list = ['SP500', 'NASDAQCOM', 'DJIA','BOGMBASEW', 'DEXJPUS', 'DEXUSEU', 'DEXCHUS', 'DEXUSAL','VIXCLS','USDONTD156N', \n",
    "               'USD1MTD156N', 'USD3MTD156N', 'USD12MD156N',\n",
    "    'BAMLHYH0A0HYM2TRIV', 'BAMLCC0A1AAATRIV','GOLDAMGBD228NLBM', \n",
    "    'DCOILWTICO','MHHNGSP','VXXLECLS'] # cboe energy sector etf volatility\n",
    "start = datetime(2015, 1, 1)\n",
    "end = datetime.now()\n",
    "    \n",
    "def get_fred_data(series_list, start, end):\n",
    "        fred_df = pd.DataFrame()\n",
    "        for i, series in enumerate(series_list):\n",
    "            print('Calling FRED API for Series:  {}'.format(series)),\n",
    "            if i == 0:\n",
    "                fred_df = web.get_data_fred(series, start, end)\n",
    "            else:\n",
    "                _df = web.get_data_fred(series, start, end)\n",
    "                fred_df = fred_df.join(_df, how='outer')\n",
    "        return fred_df\n",
    "\n",
    "econ_df = get_fred_data(series_list, start, end)\n",
    "econ_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_holiday_week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>January</td>\n",
       "      <td>2015</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              month  year   weekday  is_weekday  is_holiday  is_holiday_week\n",
       "date                                                                        \n",
       "2015-01-01  January  2015  Thursday           1           1                1\n",
       "2015-01-02  January  2015    Friday           1           0                1\n",
       "2015-01-03  January  2015  Saturday           0           0                1\n",
       "2015-01-04  January  2015    Sunday           0           0                1\n",
       "2015-01-05  January  2015    Monday           1           0                0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def generate_calendar(year, drop_index=False):\n",
    "    from pandas.tseries.offsets import YearEnd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    start_date = pd.to_datetime('1/1/'+str(year))\n",
    "    end_date = start_date + YearEnd()\n",
    "    DAT = pd.date_range(str(start_date), str(end_date), freq='D')\n",
    "    MO = [d.strftime('%B') for d in DAT]\n",
    "    holidays = USFederalHolidayCalendar().holidays(start=start_date, end=end_date)\n",
    "\n",
    "    cal_df = pd.DataFrame({'date':DAT, 'month':MO})\n",
    "    cal_df['year'] = [format(d, '%Y') for d in DAT]\n",
    "    cal_df['weekday'] = [format(d, '%A') for d in DAT]\n",
    "    cal_df['is_weekday'] = cal_df.weekday.isin(['Monday','Tuesday','Wednesday','Thursday','Friday'])\n",
    "    cal_df['is_weekday'] = cal_df['is_weekday'].astype(int)\n",
    "    cal_df['is_holiday'] = cal_df['date'].isin(holidays)\n",
    "    cal_df['is_holiday'] = cal_df['is_holiday'].astype(int)\n",
    "    cal_df['is_holiday_week'] = cal_df.is_holiday.rolling(window=7,center=True,min_periods=1).sum()\n",
    "    cal_df['is_holiday_week'] = cal_df['is_holiday_week'].astype(int)\n",
    "    \n",
    "    if not drop_index: cal_df.set_index('date', inplace=True)\n",
    "    \n",
    "    return cal_df\n",
    "    \n",
    "def make_calendars(year_list, drop_index):\n",
    "    cal_df = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        cal_df = cal_df.append(generate_calendar(year, drop_index=drop_index))\n",
    "    return cal_df\n",
    "    \n",
    "year_list = [str(int(i)) for i in np.arange(2015, 2020)]\n",
    "cal_df = make_calendars(year_list, drop_index=False)\n",
    "cal_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df = econ_df.join(cal_df, how='outer')\n",
    "econ_df = econ_df.fillna(method='bfill')\n",
    "econ_df = econ_df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "#drop future records introduced from the calendar function\n",
    "before_future = pd.to_datetime(econ_df.index.values) <= dt.now()\n",
    "econ_df = econ_df.loc[before_future]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df = pd.get_dummies(econ_df,\n",
    "                         columns=['month', 'year', 'weekday'],\n",
    "                         drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sp500', 'nasdaqcom', 'djia', 'bogmbasew', 'dexjpus', 'dexuseu', 'dexchus', 'dexusal', 'vixcls', 'usdontd156n', 'usd1mtd156n', 'usd3mtd156n', 'usd12md156n', 'bamlhyh0a0hym2triv', 'bamlcc0a1aaatriv', 'goldamgbd228nlbm', 'dcoilwtico', 'mhhngsp', 'vxxlecls', 'is_weekday', 'is_holiday', 'is_holiday_week', 'month_august', 'month_december', 'month_february', 'month_january', 'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', 'month_september', 'year_2016', 'year_2017', 'year_2018', 'year_2019', 'weekday_monday', 'weekday_saturday', 'weekday_sunday', 'weekday_thursday', 'weekday_tuesday', 'weekday_wednesday']\n"
     ]
    }
   ],
   "source": [
    "econ_df.columns = [str.lower(s) for s in econ_df.columns]\n",
    "print(econ_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original data to a dictionary\n",
    "data = dict()\n",
    "data['original'] = econ_df\n",
    "econ_df.to_csv('econ_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "   \n",
    "Two methods were undertaken to reduce the noise and spread the signal thruogh time in the data.  Rather than using the raw price data we do the following:\n",
    "   \n",
    "- Melt the data such that only three columns exist: `date`, `variable`, and `value`.\n",
    "- Perform a split-apply-combine by grouping the data by `variable`, calculate a percent change, then calculate a rolling `window` mean of the percent change\n",
    "    \"- Spread the data back to its original shape, using the rolling `window` percent change as the new features\n",
    "   \n",
    "This technique is an attempt to be more sensitive to changes in a given market as opposed to the actual value at any given time.  Taking a rolling mean also spreads out any market movements that may be anomalous such that they are more in the ballpark.\n",
    "\n",
    " ## Melt `econ_df` on `date` Column\n",
    " To simplify plotting and facility the split-apply-combine operation the `econ_df` is melted on the `date` column.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>sp500</td>\n",
       "      <td>2058.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>sp500</td>\n",
       "      <td>2058.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>sp500</td>\n",
       "      <td>2020.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>sp500</td>\n",
       "      <td>2020.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>sp500</td>\n",
       "      <td>2020.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date variable    value\n",
       "0 2015-01-01    sp500  2058.20\n",
       "1 2015-01-02    sp500  2058.20\n",
       "2 2015-01-03    sp500  2020.58\n",
       "3 2015-01-04    sp500  2020.58\n",
       "4 2015-01-05    sp500  2020.58"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ_df_melt = econ_df.copy()\n",
    "econ_df_melt.reset_index(inplace=True)\n",
    "econ_df_melt.rename(columns={'index': 'date'}, inplace=True)\n",
    "econ_df_melt = econ_df_melt.melt('date')\n",
    "econ_df_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-Apply-Combine을 수행하여 형상 계산\n",
    "\"Below we define the `window`, then split `econ_df_melt` on the `variable` column (which contains the names of the original columns).  The list of `onehot_cols` are not subject to this calculation since they are binary \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_cols = ['is_weekday', 'is_holiday', 'is_holiday_week', 'month_august', 'month_december',\n",
    "            'month_february', 'month_january', 'month_july',\n",
    "                   'month_june', 'month_march',\n",
    "                  'month_may', 'month_november', 'month_october',\n",
    "                  'month_september', 'year_2011', \n",
    "                 'year_2012', 'year_2013', 'year_2014', 'year_2015', \n",
    "              'year_2016', 'year_2017', \n",
    "                  'year_2018', 'weekday_monday', 'weekday_saturday', \n",
    "               'weekday_sunday', 'weekday_thursday','weekday_tuesday', 'weekday_wednesday']\n",
    "    \n",
    "window = 30 #rolling avg\n",
    "smooth_df = pd.DataFrame()\n",
    "for name, df in econ_df_melt.groupby('variable'):\n",
    "    if name not in onehot_cols:\n",
    "        colname = 'rolling_'+str(window)+'_mean'\n",
    "        df['pct_change'] = df['value'].pct_change()\n",
    "        df[colname] = df['pct_change'].rolling(window=window).mean()\n",
    "    else:\n",
    "                df[colname] = df['value']\n",
    "                smooth_df = smooth_df.append(df)\n",
    "                smooth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "yearsFmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "def plot_tseries_over_group_with_histograms(df, xcol,\n",
    "                                            ycol,grpcol,title_prepend='{}',\n",
    "                                            labs=None, x_angle=0, labelpad=60, window=15, ignore_cols=[]):\n",
    "    #Function for plotting time series df[ycol] over datetime range df[xcol]\n",
    "    #df: pd.DataFrame containing datetime and series to plot\n",
    "    #- xcol: str of column name in df for datetime series\n",
    "    #- ycol: str of column name in df for tseries\n",
    "    #- grpcol: str of column name in df of group over which to plot\n",
    "    #- labs: dict of xlab, ylab\n",
    "    # - title_prepend: str containing \\\"{}\\\" that prepends group names in title\n",
    "    #  - window: int for calculating rolling means of each series\n",
    "    # - ignore_cols: list of column names not to plot\n",
    "    \n",
    "    unique_grp_vals = df[grpcol].unique()\n",
    "    nrows = len(unique_grp_vals) - len(ignore_cols)\n",
    "    figsize = (13, 6 * nrows)\n",
    "    fig, axes = plt.subplots(nrows, 1, figsize=figsize)\n",
    "    title_prepend_hist = 'Histogram of ' + str(title_prepend)\n",
    "    j = 0\n",
    "    for i, grp in enumerate(unique_grp_vals):\n",
    "        _df = df.loc[df[grpcol] == grp]\n",
    "        if grp not in ignore_cols:\n",
    "                 _df = df.loc[df[grpcol] == grp]\n",
    "        try:\n",
    "                    ax = axes[j]\n",
    "                    ax.plot(_df[xcol], _df[ycol], alpha=.2, color='black')\n",
    "                    ax.plot(_df[xcol], _df[ycol].rolling(window=window, min_periods=min(5, window)).mean(),alpha=.5, color='r', label='{} period rolling avg'.format(window),linestyle='--')\n",
    "                    longer_window = int(window * 3)\n",
    "                    ax.plot(_df[xcol], _df[ycol].rolling(window=longer_window, min_periods=5).mean(),alpha=.8, color='darkred', label='{} period rolling avg'.format(longer_window),linewidth=2),\n",
    "                    mu, sigma = _df[ycol].mean(), _df[ycol].std()\n",
    "                    ax.axhline(mu, linestyle='--', color='r', alpha=.3)\n",
    "                    ax.axhline(mu - sigma, linestyle='-.', color='y', alpha=.3)\n",
    "                    ax.axhline(mu + sigma, linestyle='-.', color='y', alpha=.3)\n",
    "                    ax.set_title(title_prepend.format(grp))\n",
    "                    ax.legend(loc='best')\n",
    "                    bottom, top = mu - 3*sigma, mu + 3*sigma\n",
    "                    ax.set_ylim((bottom, top))\n",
    "                    if labs is not None:\n",
    "                        ax.set_xlabel(labs['xlab'])\n",
    "                        ax.set_ylabel(labs['ylab'])\n",
    "                        ax.xaxis.labelpad = labelpad\n",
    "                        ax.xaxis.set_minor_locator(months)\n",
    "                        ax.grid(alpha=.1)\n",
    "                    if x_angle != 0:\n",
    "                        for tick in ax.get_xticklabels():\n",
    "                            tick.set_rotation(x_angle)\n",
    "    \n",
    "                    divider = make_axes_locatable(ax)\n",
    "                    axHisty = divider.append_axes('right', 1.2, pad=0.1, sharey=ax)\n",
    "                    axHisty.grid(alpha=.1)\n",
    "                    axHisty.hist(_df[ycol].dropna(), orientation='horizontal', alpha=.5, color='lightgreen', bins=25)\n",
    "                    axHisty.axhline(mu, linestyle='--', color='r', label='mu', alpha=.3)\n",
    "                    axHisty.axhline(mu - sigma, linestyle='-.', color='y', label='+/- two sigma', alpha=.3)\n",
    "                    axHisty.axhline(mu + sigma, linestyle='-.', color='y', alpha=.3)\n",
    "                    axHisty.legend(loc='best')\n",
    "    \n",
    "                    j += 1\n",
    "        except IndexError:\n",
    "                        pass\n",
    "        else: \n",
    "                        pass\n",
    "                \n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.despine()\n",
    "    \n",
    "        title_prepend = 'Time Series for {}'\n",
    "        xcol = 'date'\n",
    "        ycol = colname # from the rolling mean of pct change\n",
    "        grpcol = 'variable'\n",
    "        labs = dict(xlab='',ylab=str(window)+' Day Rolling Mean of Daily Percent Change')\n",
    "    \n",
    "        plot_tseries_over_group_with_histograms(smooth_df,\n",
    "                                            xcol, ycol, grpcol,\n",
    "                                            title_prepend, labs,\n",
    "                                            x_angle=90,\n",
    "                                            ignore_cols=onehot_cols,\n",
    "                                            window=50)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_holiday_week</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>month_august</th>\n",
       "      <th>month_december</th>\n",
       "      <th>month_february</th>\n",
       "      <th>month_january</th>\n",
       "      <th>month_july</th>\n",
       "      <th>month_june</th>\n",
       "      <th>month_march</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_november</th>\n",
       "      <th>month_october</th>\n",
       "      <th>month_september</th>\n",
       "      <th>weekday_monday</th>\n",
       "      <th>weekday_saturday</th>\n",
       "      <th>weekday_sunday</th>\n",
       "      <th>weekday_thursday</th>\n",
       "      <th>weekday_tuesday</th>\n",
       "      <th>weekday_wednesday</th>\n",
       "      <th>year_2016</th>\n",
       "      <th>year_2017</th>\n",
       "      <th>year_2018</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variable    is_holiday  is_holiday_week  is_weekday  month_august  \\\n",
       "date                                                                \n",
       "2015-01-01         1.0              1.0         1.0           0.0   \n",
       "2015-01-02         0.0              1.0         1.0           0.0   \n",
       "2015-01-03         0.0              1.0         0.0           0.0   \n",
       "2015-01-04         0.0              1.0         0.0           0.0   \n",
       "2015-01-05         0.0              0.0         1.0           0.0   \n",
       "\n",
       "variable    month_december  month_february  month_january  month_july  \\\n",
       "date                                                                    \n",
       "2015-01-01             0.0             0.0            1.0         0.0   \n",
       "2015-01-02             0.0             0.0            1.0         0.0   \n",
       "2015-01-03             0.0             0.0            1.0         0.0   \n",
       "2015-01-04             0.0             0.0            1.0         0.0   \n",
       "2015-01-05             0.0             0.0            1.0         0.0   \n",
       "\n",
       "variable    month_june  month_march  month_may  month_november  month_october  \\\n",
       "date                                                                            \n",
       "2015-01-01         0.0          0.0        0.0             0.0            0.0   \n",
       "2015-01-02         0.0          0.0        0.0             0.0            0.0   \n",
       "2015-01-03         0.0          0.0        0.0             0.0            0.0   \n",
       "2015-01-04         0.0          0.0        0.0             0.0            0.0   \n",
       "2015-01-05         0.0          0.0        0.0             0.0            0.0   \n",
       "\n",
       "variable    month_september  weekday_monday  weekday_saturday  weekday_sunday  \\\n",
       "date                                                                            \n",
       "2015-01-01              0.0             0.0               0.0             0.0   \n",
       "2015-01-02              0.0             0.0               0.0             0.0   \n",
       "2015-01-03              0.0             0.0               1.0             0.0   \n",
       "2015-01-04              0.0             0.0               0.0             1.0   \n",
       "2015-01-05              0.0             1.0               0.0             0.0   \n",
       "\n",
       "variable    weekday_thursday  weekday_tuesday  weekday_wednesday  year_2016  \\\n",
       "date                                                                          \n",
       "2015-01-01               1.0              0.0                0.0        0.0   \n",
       "2015-01-02               0.0              0.0                0.0        0.0   \n",
       "2015-01-03               0.0              0.0                0.0        0.0   \n",
       "2015-01-04               0.0              0.0                0.0        0.0   \n",
       "2015-01-05               0.0              0.0                0.0        0.0   \n",
       "\n",
       "variable    year_2017  year_2018  \n",
       "date                              \n",
       "2015-01-01        0.0        0.0  \n",
       "2015-01-02        0.0        0.0  \n",
       "2015-01-03        0.0        0.0  \n",
       "2015-01-04        0.0        0.0  \n",
       "2015-01-05        0.0        0.0  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    smooth_df = smooth_df.pivot(index='date',\n",
    "                            columns='variable', \n",
    "                            values=colname)\n",
    "    smooth_df.dropna(inplace=True)\n",
    "\n",
    "    smooth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cols = ['bamlcc0a1aaatriv', 'bamlhyh0a0hym2triv', 'bogmbasew', \n",
    "            'dcoilwtico','dexchus', 'dexjpus', 'dexusal', 'dexuseu', 'djia', \n",
    "            'goldamgbd228nlbm', 'mhhngsp', 'nasdaqcom', 'sp500', 'usd12md156n', \n",
    "            'usd1mtd156n','usd3mtd156n', 'usdontd156n', 'vixcls', 'vxxlecls']\n",
    "  \n",
    "def correlation_heatmap(df, cutoff=None, title=''):\n",
    "        df_corr = df.corr('pearson')\n",
    "        np.fill_diagonal(df_corr.values, 0)\n",
    "        if cutoff != None:\n",
    "            for col in df_corr.columns:\n",
    "                df_corr.loc[df_corr[col].abs() <= cutoff, col] = 0\n",
    "        fig, ax = plt.subplots(figsize=(20, 15))\n",
    "        sns.heatmap(df_corr, ax=ax, cmap='RdBu_r')\n",
    "        plt.suptitle(title, size=18)\n",
    "        plt.show()\n",
    "        return df_corr\n",
    "cutoff = .3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/smooth_df_30_mean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-a4c76851c699>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# write to disk \\n\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/smooth_df_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_mean.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msmooth_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'smooth_df'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msmooth_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/smooth_df_30_mean.csv'"
     ]
    }
   ],
   "source": [
    "y_col = 'dcoilwtico'\n",
    "  \n",
    "     #map the values to the corresponding dates \n",
    "    # in the moving averages dataset\\n\",\n",
    "y_dict = econ_df[y_col].to_dict()\n",
    "smooth_df[y_col] = smooth_df.index.values\n",
    "smooth_df[y_col] = smooth_df[y_col].map(y_dict)\n",
    "   \n",
    "    # shift back -window so we are predicting +window in future\\n\",\n",
    "smooth_df[y_col] = smooth_df[y_col].shift(-window)\n",
    "smooth_df.dropna(inplace=True)\n",
    "    \n",
    "# write to disk \\n\",\n",
    "fname = './data/smooth_df_'+str(window)+'_mean.csv'\n",
    "smooth_df.to_csv(fname)\n",
    "data['smooth_df'] = smooth_df\n",
    "smooth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fred = Fred(api_key= '7da0b3b06d3293541808e737b9ce9add')\n",
    "data = fred.get_series('DCOILWTICO')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
